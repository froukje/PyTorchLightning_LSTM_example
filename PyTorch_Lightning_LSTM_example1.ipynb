{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "* Dataset from: https://www.kaggle.com/uciml/electric-power-consumption-data-set\n",
    "* Use PyTorchLightning to train and predict a model for 'Global_active_power'\n",
    "    * Create a Datset for the time series\n",
    "    * Create a Data Module\n",
    "    * Create a model\n",
    "* Use the information from the notebook 'DataAnalysis'\n",
    "* The notebook has been inspired by: https://www.kaggle.com/tartakovsky/pytorch-lightning-lstm-timeseries-clean-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowConDataSet(Dataset):\n",
    "    def __init__(self, X, y, seq_len=1):\n",
    "        \n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx:idx+self.seq_len-1], self.y[idx+self.seq_len-1])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len-1)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowConDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, seq_len = 1, batch_size = 128, num_workers=8):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "    def setup(self, stage):\n",
    "        '''\n",
    "        * read Data\n",
    "        * 'Date' and 'Time' columns are merged into 'date' index\n",
    "        * convert all to float and delete nans\n",
    "        * resampled to hourly intervals\n",
    "        * define X (features) and y (lables)\n",
    "        '''\n",
    "        # read data\n",
    "        filepath = 'data/household_power_consumption.txt'\n",
    "        df_powcon = pd.read_csv(filepath, sep=';',\n",
    "                        parse_dates={'date':['Date','Time']},\n",
    "                        infer_datetime_format=True,\n",
    "                        index_col='date')\n",
    "        \n",
    "        # change types to float (and all no number values to nan)\n",
    "        for i in range(len(df_powcon.columns)):\n",
    "            df_powcon.iloc[:,i] = pd.to_numeric(df_powcon.iloc[:,i], errors='coerce')\n",
    "        \n",
    "        # resamble to hourly means\n",
    "        df_powcon = df_powcon.resample('h').mean()\n",
    "        \n",
    "        df_powcon.dropna(inplace=True)\n",
    "        df_powcon = df_powcon.astype(float)\n",
    "        \n",
    "        # define features (X) and labels (y)\n",
    "        y = df_powcon['Global_active_power'].values\n",
    "\n",
    "        columns = ['Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "        X = np.zeros((len(columns), len(y)))\n",
    "        X = df_powcon[columns].values\n",
    "        \n",
    "        # train - valid - test splits\n",
    "        X_tmp, self.X_test, y_tmp, self.y_test = train_test_split(X, y, shuffle=False, test_size=.2)\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X_tmp, y_tmp, shuffle=False, test_size=.25)\n",
    "        \n",
    "        # normalize each column\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(self.X_train)\n",
    "            \n",
    "        self.X_train = scaler.transform(self.X_train)\n",
    "        self.X_val = scaler.transform(self.X_val)\n",
    "        self.X_test = scaler.transform(self.X_test)\n",
    "        self.y_train = self.y_train.reshape(-1,1)\n",
    "        self.y_val = self.y_val.reshape(-1,1)\n",
    "        self.y_test = self.y_test.reshape(-1,1)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        '''\n",
    "        * no further transformation necessary\n",
    "        * wrap dataset in dataloader\n",
    "        '''\n",
    "        # create dataset\n",
    "        train_dataset = PowConDataSet(self.X_train, self.y_train, seq_len=self.seq_len)\n",
    "        \n",
    "        # wrap dataset in dataloader\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size = self.batch_size, shuffle = False, \n",
    "                                      num_workers = self.num_workers)\n",
    "        \n",
    "        return train_dataloader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # create dataset\n",
    "        val_dataset = PowConDataSet(self.X_val, self.y_val, seq_len=self.seq_len)\n",
    "        \n",
    "        # wrap dataset in dataloader\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size = self.batch_size, shuffle = False,\n",
    "                                   num_workers = self.num_workers)\n",
    "        \n",
    "        return val_dataloader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        # create dataset\n",
    "        test_dataset = PowConDataSet(self.X_test, self.y_test, seq_len=self.seq_len)\n",
    "        \n",
    "        # wrap dataset in dataloader\n",
    "        val_dataloader = DataLoader(test_dataset, batch_size = self.batch_size, shuffle = False,\n",
    "                                   num_workers = self.num_workers)\n",
    "        \n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowConModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, n_features, hidden_size, seq_len,\n",
    "                 batch_size, num_layers, dropout,\n",
    "                 learning_rate, criterion):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=n_features,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=dropout,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x) # lstm_out = (batch_size, seq_len, hidden_size)\n",
    "        x = self.fc(lstm_out[:,-1])\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        #result = pl.TrainResult(minimize=loss)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        #result = pl.EvalResult(checkpoint_on=loss)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat,y)\n",
    "        #result = pl.EvalResult()\n",
    "        self.log('test_loss', loss)\n",
    "        return loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dict(\n",
    "    seq_len = 24,\n",
    "    batch_size = 128, \n",
    "    criterion = nn.MSELoss(),\n",
    "    max_epochs = 1,\n",
    "    n_features = 6,\n",
    "    hidden_size = 10,\n",
    "    num_layers = 1,\n",
    "    dropout = 0.2,\n",
    "    learning_rate = 0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frauke/anaconda3/envs/lightning_env/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "/home/frauke/anaconda3/envs/lightning_env/lib/python3.7/site-packages/pytorch_lightning/accelerators/cpu_accelerator.py:44: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  self.trainer.call_setup_hook(model)\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | criterion | MSELoss | 0     \n",
      "1 | lstm      | LSTM    | 720   \n",
      "2 | fc        | Linear  | 11    \n",
      "--------------------------------------\n",
      "731       Trainable params\n",
      "0         Non-trainable params\n",
      "731       Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  75%|███████▍  | 160/214 [00:03<00:01, 42.79it/s, loss=0.502, v_num=8, train_loss=0.557] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0:  78%|███████▊  | 166/214 [00:03<00:01, 42.10it/s, loss=0.502, v_num=8, train_loss=0.557]\n",
      "Epoch 0:  86%|████████▌ | 183/214 [00:04<00:00, 45.20it/s, loss=0.502, v_num=8, train_loss=0.557]\n",
      "Epoch 0:  94%|█████████▍| 202/214 [00:04<00:00, 48.67it/s, loss=0.502, v_num=8, train_loss=0.557]\n",
      "Epoch 0: 100%|██████████| 214/214 [00:04<00:00, 50.36it/s, loss=0.502, v_num=8, train_loss=0.386]\n",
      "Epoch 0: 100%|██████████| 214/214 [00:04<00:00, 50.21it/s, loss=0.502, v_num=8, train_loss=0.386]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = PowConDataModule(seq_len = p['seq_len'],\n",
    "                           batch_size = p['batch_size'])\n",
    "\n",
    "model = PowConModel(n_features = p['n_features'],\n",
    "                    hidden_size = p['hidden_size'],\n",
    "                    seq_len = p['seq_len'],\n",
    "                    batch_size = p['batch_size'],\n",
    "                    criterion = p['criterion'],\n",
    "                    num_layers = p['num_layers'],\n",
    "                    dropout = p['dropout'],\n",
    "                    learning_rate = p['learning_rate'])\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=p['max_epochs'])\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
